{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "from datasets import load_impressionv2_dataset_split\n",
    "from mutils import ElapsedTime\n",
    "\n",
    "from models import MULTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_impressionv2_dataset_split(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = th.utils.data.DataLoader(train_ds, batch_size=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, face, text, label = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='MOSEI Sentiment Analysis')\n",
    "parser.add_argument('-f', default='', type=str)\n",
    "\n",
    "# Fixed\n",
    "parser.add_argument('--model', type=str, default='MulT',\n",
    "                    help='name of the model to use (Transformer, etc.)')\n",
    "\n",
    "# Tasks\n",
    "parser.add_argument('--vonly', action='store_true',\n",
    "                    help='use the crossmodal fusion into v (default: False)')\n",
    "parser.add_argument('--aonly', action='store_true',\n",
    "                    help='use the crossmodal fusion into a (default: False)')\n",
    "parser.add_argument('--lonly', action='store_true',\n",
    "                    help='use the crossmodal fusion into l (default: False)')\n",
    "parser.add_argument('--aligned', action='store_true',\n",
    "                    help='consider aligned experiment or not (default: False)')\n",
    "parser.add_argument('--dataset', type=str, default='mosei_senti',\n",
    "                    help='dataset to use (default: mosei_senti)')\n",
    "parser.add_argument('--data_path', type=str, default='data',\n",
    "                    help='path for storing the dataset')\n",
    "\n",
    "# Dropouts\n",
    "parser.add_argument('--attn_dropout', type=float, default=0.1,\n",
    "                    help='attention dropout')\n",
    "parser.add_argument('--attn_dropout_a', type=float, default=0.0,\n",
    "                    help='attention dropout (for audio)')\n",
    "parser.add_argument('--attn_dropout_v', type=float, default=0.0,\n",
    "                    help='attention dropout (for visual)')\n",
    "parser.add_argument('--relu_dropout', type=float, default=0.1,\n",
    "                    help='relu dropout')\n",
    "parser.add_argument('--embed_dropout', type=float, default=0.25,\n",
    "                    help='embedding dropout')\n",
    "parser.add_argument('--res_dropout', type=float, default=0.1,\n",
    "                    help='residual block dropout')\n",
    "parser.add_argument('--out_dropout', type=float, default=0.0,\n",
    "                    help='output layer dropout')\n",
    "\n",
    "# Architecture\n",
    "parser.add_argument('--nlevels', type=int, default=5,\n",
    "                    help='number of layers in the network (default: 5)')\n",
    "parser.add_argument('--num_heads', type=int, default=5,\n",
    "                    help='number of heads for the transformer network (default: 5)')\n",
    "parser.add_argument('--attn_mask', action='store_false',\n",
    "                    help='use attention mask for Transformer (default: true)')\n",
    "\n",
    "# Tuning\n",
    "parser.add_argument('--batch_size', type=int, default=24, metavar='N',\n",
    "                    help='batch size (default: 24)')\n",
    "parser.add_argument('--clip', type=float, default=0.8,\n",
    "                    help='gradient clip value (default: 0.8)')\n",
    "parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                    help='initial learning rate (default: 1e-3)')\n",
    "parser.add_argument('--optim', type=str, default='Adam',\n",
    "                    help='optimizer to use (default: Adam)')\n",
    "parser.add_argument('--num_epochs', type=int, default=40,\n",
    "                    help='number of epochs (default: 40)')\n",
    "parser.add_argument('--when', type=int, default=20,\n",
    "                    help='when to decay learning rate (default: 20)')\n",
    "parser.add_argument('--batch_chunk', type=int, default=1,\n",
    "                    help='number of chunks per batch (default: 1)')\n",
    "\n",
    "# Logistics\n",
    "parser.add_argument('--log_interval', type=int, default=30,\n",
    "                    help='frequency of result logging (default: 30)')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--no_cuda', action='store_true',\n",
    "                    help='do not use cuda')\n",
    "parser.add_argument('--name', type=str, default='mult',\n",
    "                    help='name of the trial (default: \"mult\")')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = args\n",
    "hyp_params.orig_d_l = text.shape[2]\n",
    "hyp_params.orig_d_a = audio.shape[2]\n",
    "hyp_params.orig_d_v = face.shape[2]\n",
    "hyp_params.l_len = text.shape[1]\n",
    "hyp_params.a_len = audio.shape[1]\n",
    "hyp_params.v_len = face.shape[1]\n",
    "hyp_params.layers = args.nlevels\n",
    "hyp_params.use_cuda = True\n",
    "# hyp_params.dataset = dataset\n",
    "hyp_params.when = args.when\n",
    "hyp_params.batch_chunk = args.batch_chunk\n",
    "# hyp_params.n_train, hyp_params.n_valid, hyp_params.n_test = len(train_data), len(valid_data), len(test_data)\n",
    "# hyp_params.model = str.upper(args.model.strip())\n",
    "hyp_params.output_dim = label.shape[1] #output_dim_dict.get(dataset, 1)\n",
    "hyp_params.criterion = th.nn.L1Loss #criterion_dict.get(dataset, 'L1Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MULTModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-04715a4ae70f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/transformer/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_l, x_a, x_v)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Project the textual/visual/audio features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mproj_x_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_l\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_d_l\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_l\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mproj_x_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_a\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_d_a\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_a\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mproj_x_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_v\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_d_v\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_v\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mproj_x_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproj_x_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    257\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    258\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 259\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "pred = model(text, audio, face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
